{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "from os.path import join\n",
    "import pickle\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import tagme\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/rohitalyosha/Student_Job/mannheim-nel/data'\n",
    "datasets = ['conll-train', 'conll-dev', 'msnbc', 'ace2004']\n",
    "tagme.GCUBE_TOKEN = \"88c693df-a43f-4086-b3bc-0b555bfbc9bb-843339462\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = json_load(join(data_path, 'dicts/redirects.json'))\n",
    "ent2id = json_load('/home/rohitalyosha/Student_Job/mannheim-nel/data/dicts/ent_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2c = {}\n",
    "id2c_conll = pickle_load(join(data_path, 'Conll', 'conll_raw_text.pickle'))\n",
    "id2c['conll-train'] = id2c_conll['train']\n",
    "id2c['conll-dev'] = id2c_conll['dev']\n",
    "examples = {}\n",
    "\n",
    "for d_name in datasets[2:]:\n",
    "    id2c[d_name], examples[d_name] = pickle_load(join(data_path, 'datasets', f'raw_{d_name}.pickle'))\n",
    "\n",
    "for d_name in datasets[:2]:\n",
    "    _, examples[d_name] = pickle_load(join(data_path, 'Conll', f\"conll-{d_name.split('-')[-1]}.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = {dataset : {} for dataset in datasets}\n",
    "for dataset, exs in examples.items():\n",
    "    for ex in exs:\n",
    "        c_id, (mention, ent_str, span, _) = ex\n",
    "        if c_id not in gold[dataset]:\n",
    "            gold[dataset][c_id] = {'mentions': [],\n",
    "                          'ents': [],\n",
    "                          'spans': []}\n",
    "        gold[dataset][c_id]['mentions'].append(mention)\n",
    "        gold[dataset][c_id]['ents'].append(ent_str)\n",
    "        gold[dataset][c_id]['spans'].append(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_mention(doc_id, ent_strs, text, user_mentions, user_spans):\n",
    "    data_json = json.dumps({'ent_strs': ent_strs,\n",
    "                            'doc_id': doc_id,\n",
    "                            'text': text,\n",
    "                            'mentions': user_mentions, \n",
    "                            'spans': user_spans})\n",
    "    response_json = requests.post(\"http://127.0.0.1:5000/link\", data=data_json).json()\n",
    "    ents = response_json['entities']\n",
    "    mentions = response_json['mentions']\n",
    "    \n",
    "    return ents, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_results(num_text, dataset='conll-dev'):\n",
    "    results = {}\n",
    "    times = []\n",
    "    for doc_id, text in list(id2c[dataset].items())[:num_text]:\n",
    "        if doc_id not in gold[dataset]:\n",
    "            continue\n",
    "        results[doc_id] = {}\n",
    "        user_mentions = gold[dataset][doc_id]['mentions']\n",
    "        user_spans = gold[dataset][doc_id]['spans']\n",
    "        user_ents = gold[dataset][doc_id]['ents']\n",
    "        print(user_ents)\n",
    "        try:\n",
    "            ents, mentions = get_response_mention(f'{dataset}-{doc_id}', user_ents, text, user_mentions, user_spans)\n",
    "        except Exception as e:\n",
    "            print(Text, user_mentions)\n",
    "        results[doc_id]['mentions'] = mentions\n",
    "        results[doc_id]['ents'] = ents\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval only linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ace2004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Baghdad', 'Agence_France-Presse', 'Iraq']\n",
      "['Tallahassee,_Florida', 'Florida', 'Supreme_Court_of_Florida', 'Florida_District_Courts_of_Appeal', 'Electoral_College_(United_States)']\n",
      "['BBC_News', 'London', 'Supreme_Court_of_the_United_States', 'Ruth_Bader_Ginsburg', 'Supreme_Court_of_Florida', \"Sandra_Day_O'Connor\", 'Washington,_D.C.']\n",
      "['Bandar_Seri_Begawan', 'Agence_France-Presse', 'Florida', 'Brunei']\n",
      "['Washington,_D.C.', 'Supreme_Court_of_the_United_States', 'Joel_Grossman', 'Baltimore', 'Johns_Hopkins_University', 'Laurence_Tribe', 'Al_Gore', 'Florida_Legislature', 'United_States_Congress']\n",
      "['Xinhua_News_Agency', 'Beijing', 'Weir_Group', 'United_States', 'Jiangsu', 'China', 'Rudong_County', 'Japan', 'South_Korea', 'Taiwan']\n",
      "['Beirut', 'Agence_France-Presse', 'Lebanon', 'Israel']\n",
      "['Zeist', 'Holland', 'Agence_France-Presse', 'Soviet_Union', 'Sweden', 'Amsterdam', 'Copenhagen', 'Stockholm', 'Lamin_Khalifah_Fhimah', 'Abdelbaset_al-Megrahi', 'Malta']\n",
      "['Bandar_Seri_Begawan', 'Agence_France-Presse', 'Brunei', 'Iran']\n",
      "['American_Broadcasting_Company', 'Fox_Sports_(United_States)', 'Sun_Microsystems', 'Fidelity_Investments', 'Ocean_Spray_(cooperative)', 'InStyle', 'PepsiCo', 'Jack_Nicholson', 'Helen_Hunt', 'Greg_Kinnear', 'James_L._Brooks', 'Campbell_Soup_Company', 'Roundabout_Theatre_Company', 'Times_Square', \"Reader's_Digest_Association\", \"Reader's_Digest\", 'Levi_Strauss_&_Co.', 'Blue_Cross_Blue_Shield_Association', 'MSN', 'Microsoft', 'Land_Rover', 'Volvo', 'Sprint_Nextel', 'John_Corbett_(actor)', 'Houston_Grand_Opera', 'CBS', 'Brooklyn']\n",
      "['Xinhua_News_Agency', 'Shanghai', 'HSBC', 'International_Ocean_Shipping_Building', 'Lujaizui', 'Pudong', 'United_States', 'Citibank', 'Hong_Kong', 'Japan', 'The_Bank_of_Tokyo-Mitsubishi_UFJ']\n",
      "['United_States', 'Madeleine_Albright', 'Yale_University', 'Rick_Levin', 'Yale_Center_for_the_Study_of_Globalization', 'Middle_East', 'Time_(magazine)']\n",
      "['Washington,_D.C.', 'Ralph_Nader', 'United_States', 'White_House', 'Florida', 'Oregon', 'Portland,_Oregon', 'Green_Party_of_Florida', 'Sierra_Club', 'University_of_Florida', 'Gainesville,_Florida']\n",
      "['Voice_of_America', 'United_States', 'Texas']\n",
      "['Lebanon', 'Agence_France-Presse', 'Palestinian_National_Council', 'Algeria', 'Tyre,_Lebanon', 'Beirut', 'Muhammad_al-Durrah_incident', 'Sidon', 'Lebanon', 'Jerusalem']\n",
      "['Jerusalem', 'Agence_France-Presse', 'West_Bank']\n",
      "['Cairo', 'Egypt', 'Associated_Press', 'Syria', 'President_of_Egypt', 'Iraq', 'West_Bank', 'Gaza_Strip', 'Jerusalem']\n",
      "['Grosse_Pointe_Park,_Michigan', 'Michigan', 'New_York', 'Pennsylvania', 'Michigan_Department_of_Natural_Resources', 'Detroit']\n",
      "['BBC_News', 'London', 'Taiwan', 'Los_Angeles']\n",
      "['Gaza', 'Agence_France-Presse']\n",
      "[\"O'Hare_International_Airport\", 'LaGuardia_Airport', 'New_York_City', 'Southern_United_States', 'United_States_Department_of_Transportation', 'Los_Angeles_International_Airport', 'Bob_Hope_Airport', 'John_Wayne_Airport', 'Orange_County,_California', 'California', 'Westchester_County_Airport', 'John_F._Kennedy_International_Airport', 'Federal_Aviation_Administration', 'Washington,_D.C.', 'Airline_complaints']\n",
      "['Tallahassee,_Florida', 'United_States', 'Agence_France-Presse', 'Supreme_Court_of_Florida', 'Florida']\n",
      "['Time_Warner', 'State_Street_Corporation', 'Boston', 'Sanford_Bernstein', 'The_Walt_Disney_Company', 'Capital_Cities_Communications', 'Time_Inc.', 'Warner_Communications', 'Roger_McNamee', 'Merrill_Lynch', 'NBC', 'Washington,_D.C.', 'Janus_Capital_Group', 'Comcast']\n",
      "['Jerusalem', 'Agence_France-Presse', 'Iran', 'Moscow', 'Ministry_of_Defense_and_Armed_Forces_Logistics_(Iran)']\n",
      "['Chattanooga,_Tennessee', 'Tennessee', 'Washington,_D.C.', 'Arkansas', 'White_House', 'Wisconsin', 'Iowa', 'Michigan', 'Pennsylvania', 'Green_Bay,_Wisconsin', 'Washington,_D.C.']\n",
      "['Beirut', 'Agence_France-Presse', 'Lebanon', 'United_Arab_Emirates', 'Mexico', 'Bangkok']\n",
      "['United_States_Navy', 'USS_Cole_(DDG-67)', 'Yemen']\n",
      "['NPR', 'Washington,_D.C.', 'Miami-Dade_County,_Florida', 'Democratic_Party_(United_States)', 'Palm_Beach_County,_Florida', 'Al_Gore', 'Washington_(state)', 'Seattle']\n",
      "['London', 'Agence_France-Presse', 'Baghdad', 'Riyadh', 'King_Khalid_International_Airport']\n",
      "['Bandar_Seri_Begawan', 'Agence_France-Presse', 'Russia', 'Brunei', 'Irkutsk']\n",
      "['Yasser_Arafat', 'Corey_Flintoff', 'NPR', 'Washington,_D.C.']\n",
      "['Democratic_Opposition_of_Serbia', 'Slobodan_Milošević', 'Socialist_Party_of_Serbia']\n",
      "['Gaza', 'Agence_France-Presse', 'Egypt', 'Netzarim_(settlement)']\n",
      "['Hong_Kong', 'Asia', 'International_Monetary_Fund', 'Philippines', 'Taiwan', 'Indonesia', 'Chen_Shui-bian', 'Malaysia', 'South_Korea', 'Singapore', 'Bloomberg_News', 'United_States', 'China', 'Hong_Kong_General_Chamber_of_Commerce']\n",
      "['Bandar_Seri_Begawan', 'Agence_France-Presse', 'United_States', 'White_House', 'Palestinian_National_Council']\n"
     ]
    }
   ],
   "source": [
    "mention_results = get_mention_results(10000, dataset=DATASET)\n",
    "\n",
    "num_correct = 0\n",
    "total = 0\n",
    "num_no_link = 0\n",
    "no_links = []\n",
    "\n",
    "for k, v in mention_results.items():\n",
    "    if k not in gold[DATASET]:\n",
    "        print(k, v)\n",
    "        continue\n",
    "    gold_ents = gold[DATASET][k]['ents']\n",
    "    pred_ents = v['ents']\n",
    "    gold_mentions = gold[DATASET][k]['mentions']\n",
    "    for i, (gold_ent, pred_ent) in enumerate(zip(gold_ents, pred_ents)):\n",
    "        gold_ent = rd.get(gold_ent, gold_ent)\n",
    "        pred_ent = rd.get(pred_ent, pred_ent)\n",
    "        total += 1\n",
    "        if pred_ent == 'NO LINK FOUND':\n",
    "            num_no_link += 1\n",
    "            no_links.append(gold_ent)\n",
    "        if gold_ent == pred_ent:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 257 0.8560311284046692\n"
     ]
    }
   ],
   "source": [
    "print(num_correct, total, num_correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cands Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    f_names = glob.glob(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/cands/{dataset}/*')\n",
    "    examples = []\n",
    "    for f_name in f_names:\n",
    "        doc_id = f_name.split('-')[-1]\n",
    "        try:\n",
    "            doc_id = int(doc_id)\n",
    "        except:\n",
    "            pass\n",
    "        with open(f_name) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                parts = line.split('||')\n",
    "                if len(parts) > 1:\n",
    "                    mention = parts[0]\n",
    "                    ent_str = parts[1]\n",
    "                    cand_strs = parts[2:]\n",
    "                    examples.append((doc_id, mention, ent_str, cand_strs))\n",
    "        \n",
    "    with open(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/{dataset}.pickle', 'wb') as f:\n",
    "        pickle.dump((id2c[dataset], examples), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll-train 18063 18626 0.9697734349833566\n",
      "conll-dev 4707 4838 0.9729226953286482\n",
      "msnbc 649 656 0.989329268292683\n",
      "ace2004 232 258 0.8992248062015504\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    covered = 0\n",
    "    total = 0\n",
    "    id2c, examples = pickle_load(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/{dataset}.pickle')\n",
    "    for doc_id, mention_str, ent_str, cand_gen_strs  in examples:\n",
    "        total += 1\n",
    "        ent_str = rd.get(ent_str, ent_str)\n",
    "        if ent_str in cand_gen_strs[:128]:\n",
    "            covered += 1\n",
    "    print(dataset, covered, total, covered/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mannheim-nel)",
   "language": "python",
   "name": "mannheim-nel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
