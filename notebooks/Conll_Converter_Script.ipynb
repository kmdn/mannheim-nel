{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = FileObjectStore('../data/mmaps/redirects')\n",
    "DOCSTART_MARKER = '-DOCSTART-'\n",
    "RE_WIKI_ENT = re.compile(r'.*wiki\\/(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "dict_names = ['ent_dict', 'word_dict', 'redirects', 'str_prior', 'str_cond', 'disamb', 'str_necounts']\n",
    "file_stores = {}\n",
    "for dict_name in dict_names:\n",
    "    file_stores[dict_name] = FileObjectStore(join(data_path, f'mmaps/{dict_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_training_doc(doc_id):\n",
    "    return 'test' not in doc_id\n",
    "\n",
    "\n",
    "def is_test_doc(doc_id):\n",
    "    return 'testb' in doc_id\n",
    "\n",
    "\n",
    "def is_dev_doc(doc_id):\n",
    "    return 'testa' in doc_id\n",
    "\n",
    "\n",
    "def doc_tag_for_id(doc_id):\n",
    "    if 'testa' in doc_id:\n",
    "        return 'dev'\n",
    "    elif 'testb' in doc_id:\n",
    "        return 'test'\n",
    "    return 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_docs(path, split, redirects={}):\n",
    "    if split == 'train':\n",
    "        doc_id_predicate = is_training_doc\n",
    "    elif split == 'dev':\n",
    "        doc_id_predicate = is_dev_doc\n",
    "    elif split == 'test':\n",
    "        doc_id_predicate = is_test_doc\n",
    "    else:\n",
    "        print('wrong split, exiting')\n",
    "        \n",
    "    with codecs.open(path, 'r', 'utf-8') as f:\n",
    "        doc_id = None\n",
    "        doc_tokens = None\n",
    "        doc_mentions = None\n",
    "\n",
    "        for line in f:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) > 0:\n",
    "                token = parts[0].strip()\n",
    "\n",
    "                # if this line contains a mention\n",
    "                if len(parts) >= 4 and parts[1] == 'B':\n",
    "\n",
    "                    if parts[3].strip() != '' and not parts[3].startswith('--'):\n",
    "                        try:\n",
    "                            entity = RE_WIKI_ENT.match(parts[4]).group(1)\n",
    "                        except AttributeError:\n",
    "                            print(parts[4])\n",
    "                        entity = redirects.get(entity, entity)\n",
    "                        begin = sum(len(t)+1 for t in doc_tokens)\n",
    "\n",
    "                        dodgy_tokenisation_bs_offset = 1 if re.search('[A-Za-z],',parts[2]) else 0\n",
    "\n",
    "                        position = (begin, begin + len(parts[2]) + dodgy_tokenisation_bs_offset)\n",
    "                        doc_mentions.append((entity, position))\n",
    "\n",
    "                if token.startswith(DOCSTART_MARKER):\n",
    "                    if doc_id is not None and doc_id_predicate(doc_id):\n",
    "                        yield (' '.join(doc_tokens), doc_mentions, doc_id)\n",
    "\n",
    "                    doc_id = token[len(DOCSTART_MARKER) + 2:-1]\n",
    "                    #print(doc_id)\n",
    "                    \n",
    "                    ## TODO: FIX THIS HACK\n",
    "                    if split == 'train' and doc_id[:3] == '618' and len(doc_tokens) == 510:\n",
    "                        yield (' '.join(doc_tokens), doc_mentions, doc_id)\n",
    "                    doc_tokens = []\n",
    "                    doc_mentions = []\n",
    "                elif doc_id is not None:\n",
    "                    doc_tokens.append(token)\n",
    "\n",
    "        if doc_id is not None and doc_id_predicate(doc_id):\n",
    "            yield (' '.join(doc_tokens), doc_mentions, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'dev', 'test']\n",
    "docid2context = {}\n",
    "all_examples = {split: [] for split in splits}\n",
    "\n",
    "for split in splits:\n",
    "    for context, mentions, doc_id in iter_docs('../data/Conll/AIDA-YAGO2-dataset.tsv', \n",
    "                                                split,\n",
    "                                                redirects=redirects):\n",
    "        docid2context[doc_id] = context\n",
    "        all_examples[split].append([(doc_id, context[begin:end], (begin, end), ent_str) for ent_str, (begin, end) in mentions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_resolver = HeuresticCorefResolver()\n",
    "detector = SpacyDetector()\n",
    "candidate_generator = NelCandidateGenerator(max_cands=256,\n",
    "                                            disamb=file_stores['disamb'],\n",
    "                                            redirects=file_stores['redirects'],\n",
    "                                            str_necounts=file_stores['str_necounts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ONE []\n",
      "215 State []\n",
      "515 London []\n",
      "734 Dole []\n",
      "1307testb Kansas []\n"
     ]
    }
   ],
   "source": [
    "full_training_examples = {split: [] for split in splits}\n",
    "\n",
    "for split, doc_examples in all_examples.items():\n",
    "    for examples in doc_examples:\n",
    "        text_spans = [(text, span) for _, text, span, _ in examples]\n",
    "        try:\n",
    "            doc_id = examples[0][0]\n",
    "        except:\n",
    "            print(doc_id, examples)\n",
    "        doc = Doc(docid2context[doc_id],\n",
    "                  file_stores=file_stores,\n",
    "                  detector=detector,\n",
    "                  candidate_generator=candidate_generator,\n",
    "                  coref_resolver=coref_resolver,\n",
    "                  doc_id=doc_id,\n",
    "                  text_spans=text_spans)\n",
    "        doc.gen_cands()\n",
    "        \n",
    "        for idx, (doc_id, text, span, ent_str) in enumerate(examples):\n",
    "            mention = doc.mentions[idx]\n",
    "            assert mention.text == text, (mention.text, text)\n",
    "            assert (mention.begin, mention.end) == span\n",
    "            full_training_examples[split].append('||'.join((doc_id, text, ent_str, '||'.join(mention.cands))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-15 17:38:36,855] WARNING in file: No existing mmap store found: /home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/train.index ...\n",
      "[2018-12-15 17:38:37,005] WARNING in file: No existing mmap store found: /home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/dev.index ...\n",
      "[2018-12-15 17:38:37,040] WARNING in file: No existing mmap store found: /home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/test.index ...\n",
      "[2018-12-15 17:38:37,073] WARNING in file: No existing mmap store found: /home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/id2context.index ...\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    f_store = FileObjectStore(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/{split}')\n",
    "    split_examples = full_training_examples[split]\n",
    "    f_store.save_many(zip(range(len(split_examples)), split_examples))\n",
    "f_store = FileObjectStore(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/id2context')\n",
    "f_store.save_many(docid2context.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'947testa CRICKET||LEICESTERSHIRE||Leicestershire_County_Cricket_Club||Leicestershire_County_Cricket_Club||Leicestershire||Leicestershire_(UK_Parliament_constituency)||High_Sheriff_of_Leicestershire||Leicestershire_Police||Leicestershire_and_Rutland_County_Football_Association||Leicestershire_Rugby_Union||Leicester_Town_Rifles||Leicestershire_Yeomanry||Leicestershire_Cricket_Board||Arriva_Fox_County||Royal_Leicestershire_Regiment||Leicestershire_and_Rutland_Cricket_Club||Leicestershire_Royal_Horse_Artillery||BBC_Radio_Leicester||Leicestershire_County_Council||Lord_Lieutenant_of_Leicestershire||Leicestershire_coalfield'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_examples['dev'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_store = FileObjectStore(f'/home/rohitalyosha/Student_Job/mannheim-nel/data/training_files/mmaps/dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'947testa CRICKET||LEICESTERSHIRE||Leicestershire_County_Cricket_Club||Leicestershire_County_Cricket_Club||Leicestershire||Leicestershire_(UK_Parliament_constituency)||High_Sheriff_of_Leicestershire||Leicestershire_Police||Leicestershire_and_Rutland_County_Football_Association||Leicestershire_Rugby_Union||Leicester_Town_Rifles||Leicestershire_Yeomanry||Leicestershire_Cricket_Board||Arriva_Fox_County||Royal_Leicestershire_Regiment||Leicestershire_and_Rutland_Cricket_Club||Leicestershire_Royal_Horse_Artillery||BBC_Radio_Leicester||Leicestershire_County_Council||Lord_Lieutenant_of_Leicestershire||Leicestershire_coalfield'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_store[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mannheim-nel)",
   "language": "python",
   "name": "mannheim-nel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
