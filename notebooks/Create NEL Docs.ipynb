{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "from os.path import join\n",
    "import pickle\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "from more_itertools import unique_everseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/rohitalyosha/Student_Job/mannheim-nel/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_dict = json_load('/home/rohitalyosha/Student_Job/mannheim-nel/data/dicts/disamb.json')\n",
    "necounts = json_load('/home/rohitalyosha/Student_Job/mannheim-nel/data/dicts/str_necounts.json')\n",
    "redirects = json_load('/home/rohitalyosha/Student_Job/mannheim-nel/data/dicts/redirects.json')\n",
    "ent2id = json_load('/home/rohitalyosha/Student_Job/mannheim-nel/data/dicts/ent_dict.json')\n",
    "id2ent = reverse_dict(ent2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    \n",
    "    def __init__(self, text, mention_tups, coref=True, disamb=False, necounts=None , rd=None):\n",
    "        self.text = text\n",
    "        self.mentions = [Mention(text,\n",
    "                                 ent_str,\n",
    "                                 span, \n",
    "                                 small_context,\n",
    "                                 coref=coref,\n",
    "                                 disamb=disamb,\n",
    "                                 necounts=necounts,\n",
    "                                 rd=rd) \n",
    "                         for _, (text, ent_str, span, small_context) in mention_tups]\n",
    "        self.assign_clusters()\n",
    "        \n",
    "    def assign_clusters(self):\n",
    "        chains = []\n",
    "        unchained_mentions = sorted(self.mentions, key=lambda m:m.begin, reverse=True)\n",
    "\n",
    "        #log.debug('MENTIONS: ' + ';'.join(m.text for m in unchained_mentions))\n",
    "        while unchained_mentions:\n",
    "            mention = unchained_mentions.pop(0)\n",
    "\n",
    "            potential_antecedents = [(m.text, m) for m in unchained_mentions] # if m.tag == mention.tag\n",
    "            chain = [mention]\n",
    "\n",
    "            likely_acronym = False\n",
    "\n",
    "            if mention.text.upper() == mention.text:\n",
    "                # check if our mention is an acronym of a previous mention\n",
    "                for a, m in potential_antecedents:\n",
    "                    if (''.join(p[0] for p in a.split(' ') if p).upper() == mention.text) or \\\n",
    "                       (''.join(p[0] for p in a.split(' ') if p and p[0].isupper()).upper() == mention.text):\n",
    "                        chain.insert(0, m)\n",
    "                        unchained_mentions.remove(m)\n",
    "                        likely_acronym = True\n",
    "                potential_antecedents = [(m.text, m) for m in unchained_mentions]\n",
    "\n",
    "            last = None\n",
    "            longest_mention = mention\n",
    "            while last != longest_mention and potential_antecedents:\n",
    "                # check if we are a prefix/suffix of a preceding mention\n",
    "                n = longest_mention.text.lower()\n",
    "                for a, m in potential_antecedents:\n",
    "                    na = a.lower()\n",
    "                    if (likely_acronym and mention.text == a) or \\\n",
    "                       (not likely_acronym and (na.startswith(n) or na.endswith(n) or n.startswith(na) or n.endswith(na))):\n",
    "                        chain.insert(0, m)\n",
    "                        unchained_mentions.remove(m)\n",
    "\n",
    "                last = longest_mention\n",
    "                longest_mention = sorted(chain, key=lambda m: len(m.text), reverse=True)[0]\n",
    "                potential_antecedents = [(m.text, m) for m in unchained_mentions] # if m.tag == mention.tag\n",
    "\n",
    "            for mention in chain:\n",
    "                mention.cluster_mention = longest_mention.text\n",
    "            \n",
    "    def gen_cands(self):\n",
    "        for mention in self.mentions:\n",
    "            mention.gen_cands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mention:\n",
    "    \n",
    "    def __init__(self, text, ent_str, span,\n",
    "                 small_context=None,\n",
    "                 cluster_mention=None, \n",
    "                 coref=True, \n",
    "                 disamb=False,\n",
    "                 necounts=None,\n",
    "                 rd=None):\n",
    "        \n",
    "        self.text = text\n",
    "        self.ent_str = ent_str\n",
    "        self.begin, self.end = span\n",
    "        self.small_context = small_context\n",
    "        self.cluster_mention = cluster_mention\n",
    "        self.coref = coref\n",
    "        self.disamb = disamb\n",
    "        self.necounts = necounts\n",
    "        self.rd = rd\n",
    "        self.cands = []\n",
    "        \n",
    "    def add_dismb_cands(self, mention):\n",
    "        res = []\n",
    "        mention_title = mention.title().replace(' ', '_')\n",
    "        res.append(mention_title)\n",
    "        if mention_title != self.rd.get(mention_title, mention_title):\n",
    "            res.append(self.rd[mention_title])\n",
    "        mention_disamb = mention_title + '_(disambiguation)'\n",
    "\n",
    "        if mention_title in dis_dict:\n",
    "            res.extend(dis_dict[mention_title])\n",
    "        if mention_disamb in dis_dict:\n",
    "            res.extend(dis_dict[mention_disamb])\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def gen_cands(self):\n",
    "        nfs = set()\n",
    "        nfs.update(get_normalised_forms(self.text))\n",
    "        if self.coref:\n",
    "            nfs.update(get_normalised_forms(self.cluster_mention))\n",
    "\n",
    "        [self.cands.extend(self.necounts.get(nf, [])) for nf in nfs]\n",
    "        if self.disamb:\n",
    "            [self.cands.extend(self.add_dismb_cands(nf)) for nf in nfs]\n",
    "\n",
    "        self.cands = equalize_len(list(unique_everseen(self.cands)), 100, pad='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_train = pickle_load(join(DATA_PATH, 'Conll', f'conll-{split}.pickle'))\n",
    "conll_raw = pickle_load(join(DATA_PATH, 'Conll', 'conll_raw_text.pickle'))\n",
    "id2c = conll_raw[split]\n",
    "_, examples = conll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid2mention_tups = defaultdict(list)\n",
    "for example in examples:\n",
    "    doc_id, mention_tup = example\n",
    "    docid2mention_tups[doc_id].append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cand_eval(docs, num_cands=100):\n",
    "    covered = 0\n",
    "    total = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        for mention in doc.mentions:\n",
    "            total += 1\n",
    "            # ent_str = redirects.get(mention.ent_str, mention.ent_str)\n",
    "            ent_str = mention.ent_str\n",
    "            cands = mention.cands[:num_cands]\n",
    "\n",
    "            if ent_str in cands:\n",
    "                covered += 1\n",
    "    print(covered, total)\n",
    "    return covered / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16819 18546\n",
      "Coref: False, disamb: True, num_cands: 50, result: 0.9068801897983393\n",
      "17049 18546\n",
      "Coref: False, disamb: True, num_cands: 100, result: 0.9192817858298286\n",
      "17049 18546\n",
      "Coref: False, disamb: True, num_cands: 128, result: 0.9192817858298286\n",
      "17049 18546\n",
      "Coref: False, disamb: True, num_cands: 200, result: 0.9192817858298286\n",
      "17049 18546\n",
      "Coref: False, disamb: True, num_cands: 256, result: 0.9192817858298286\n",
      "17049 18546\n",
      "Coref: False, disamb: True, num_cands: 500, result: 0.9192817858298286\n",
      "16060 18546\n",
      "Coref: False, disamb: False, num_cands: 50, result: 0.8659549228944247\n",
      "16243 18546\n",
      "Coref: False, disamb: False, num_cands: 100, result: 0.8758222797368704\n",
      "16243 18546\n",
      "Coref: False, disamb: False, num_cands: 128, result: 0.8758222797368704\n",
      "16243 18546\n",
      "Coref: False, disamb: False, num_cands: 200, result: 0.8758222797368704\n",
      "16243 18546\n",
      "Coref: False, disamb: False, num_cands: 256, result: 0.8758222797368704\n",
      "16243 18546\n",
      "Coref: False, disamb: False, num_cands: 500, result: 0.8758222797368704\n",
      "17042 18546\n",
      "Coref: True, disamb: True, num_cands: 50, result: 0.9189043459506093\n",
      "17313 18546\n",
      "Coref: True, disamb: True, num_cands: 100, result: 0.9335166612746684\n",
      "17313 18546\n",
      "Coref: True, disamb: True, num_cands: 128, result: 0.9335166612746684\n",
      "17313 18546\n",
      "Coref: True, disamb: True, num_cands: 200, result: 0.9335166612746684\n",
      "17313 18546\n",
      "Coref: True, disamb: True, num_cands: 256, result: 0.9335166612746684\n",
      "17313 18546\n",
      "Coref: True, disamb: True, num_cands: 500, result: 0.9335166612746684\n",
      "16479 18546\n",
      "Coref: True, disamb: False, num_cands: 50, result: 0.8885473956648334\n",
      "16690 18546\n",
      "Coref: True, disamb: False, num_cands: 100, result: 0.8999245120241561\n",
      "16690 18546\n",
      "Coref: True, disamb: False, num_cands: 128, result: 0.8999245120241561\n",
      "16690 18546\n",
      "Coref: True, disamb: False, num_cands: 200, result: 0.8999245120241561\n",
      "16690 18546\n",
      "Coref: True, disamb: False, num_cands: 256, result: 0.8999245120241561\n",
      "16690 18546\n",
      "Coref: True, disamb: False, num_cands: 500, result: 0.8999245120241561\n"
     ]
    }
   ],
   "source": [
    "for coref in [False, True]:\n",
    "    for disamb in [True, False]:\n",
    "        for num_cands in [50, 100, 128, 200, 256, 500]:\n",
    "            docs = [Doc(id2c[doc_id],\n",
    "                        mention_tups,\n",
    "                        coref=coref, \n",
    "                        disamb=disamb, \n",
    "                        necounts=necounts,\n",
    "                        rd=redirects) \n",
    "                    for doc_id, mention_tups in docid2mention_tups.items()]\n",
    "            for doc in docs:\n",
    "                doc.gen_cands()\n",
    "            result = run_cand_eval(docs, num_cands=num_cands)\n",
    "\n",
    "            print_str  = f'Coref: {coref}, disamb: {disamb}, '\n",
    "            print_str += f'num_cands: {num_cands}, result: {result}'\n",
    "            print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mannheim-nel)",
   "language": "python",
   "name": "mannheim-nel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
